{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "# ML Assignment 2 \u2014 Train & Evaluate 6 Classification Models\n",
        "\n",
        "**Dataset:** UCI Breast Cancer Wisconsin (Diagnostic)  \n",
        "Run each cell sequentially. Every step produces visible output so you can verify.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 1. Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "from dataclasses import asdict, dataclass\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    matthews_corrcoef,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    ConfusionMatrixDisplay,\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "print(\"All imports successful \u2705\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 2. Configuration & Paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "RANDOM_STATE = 42\n",
        "\n",
        "# Resolve paths (works whether kernel cwd is project-folder/ or model/)\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "if not (PROJECT_ROOT / \"model\").exists() and (PROJECT_ROOT.parent / \"model\").exists():\n",
        "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
        "\n",
        "MODEL_DIR = PROJECT_ROOT / \"model\"\n",
        "DATA_DIR  = PROJECT_ROOT / \"data\"\n",
        "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"PROJECT_ROOT : {PROJECT_ROOT}\")\n",
        "print(f\"MODEL_DIR    : {MODEL_DIR}\")\n",
        "print(f\"DATA_DIR     : {DATA_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 3. Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "dataset = load_breast_cancer(as_frame=True)\n",
        "X = dataset.data\n",
        "y = dataset.target\n",
        "\n",
        "print(f\"Total instances : {X.shape[0]}\")\n",
        "print(f\"Total features  : {X.shape[1]}\")\n",
        "print(f\"Target classes  : {dataset.target_names.tolist()}\")\n",
        "print(f\"\\nClass distribution:\")\n",
        "print(y.value_counts().rename({0: 'malignant', 1: 'benign'}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 4a. Preview First 10 Rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "X.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 4b. Statistical Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "X.describe().round(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 4c. Check Missing Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "missing = X.isnull().sum()\n",
        "print(\"Missing values per feature:\")\n",
        "print(missing[missing > 0] if missing.any() else \"None \u2014 dataset is clean \u2705\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 4d. Target Distribution Plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(5, 3))\n",
        "y.value_counts().plot.bar(ax=ax, color=[\"#e74c3c\", \"#2ecc71\"])\n",
        "ax.set_xticklabels([\"Malignant (0)\", \"Benign (1)\"], rotation=0)\n",
        "ax.set_ylabel(\"Count\")\n",
        "ax.set_title(\"Target Class Distribution\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 4e. Feature Correlation Heatmap (top 15 features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "top_features = X.corrwith(y).abs().sort_values(ascending=False).head(15).index.tolist()\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "sns.heatmap(X[top_features].corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\", ax=ax, square=True)\n",
        "ax.set_title(\"Correlation Heatmap \u2014 Top 15 Features (by target correlation)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 5. Train / Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "feature_names = X_train.columns.tolist()\n",
        "print(f\"Training set : {X_train.shape[0]} samples\")\n",
        "print(f\"Test set     : {X_test.shape[0]} samples\")\n",
        "print(f\"Features     : {len(feature_names)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 6. Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "def build_preprocessor(feature_names, use_scaler=False):\n",
        "    steps = [(\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
        "    if use_scaler:\n",
        "        steps.append((\"scaler\", StandardScaler()))\n",
        "    return ColumnTransformer(\n",
        "        transformers=[(\"num\", Pipeline(steps=steps), feature_names)],\n",
        "        remainder=\"drop\",\n",
        "    )\n",
        "\n",
        "@dataclass\n",
        "class ModelResult:\n",
        "    model_name: str\n",
        "    accuracy: float\n",
        "    auc: float\n",
        "    precision: float\n",
        "    recall: float\n",
        "    f1: float\n",
        "    mcc: float\n",
        "\n",
        "def compute_metrics(y_true, y_pred, y_prob, model_name):\n",
        "    return ModelResult(\n",
        "        model_name=model_name,\n",
        "        accuracy=accuracy_score(y_true, y_pred),\n",
        "        auc=roc_auc_score(y_true, y_prob),\n",
        "        precision=precision_score(y_true, y_pred),\n",
        "        recall=recall_score(y_true, y_pred),\n",
        "        f1=f1_score(y_true, y_pred),\n",
        "        mcc=matthews_corrcoef(y_true, y_pred),\n",
        "    )\n",
        "\n",
        "def train_and_evaluate(name, pipeline):\n",
        "    \"\"\"Train, print metrics, show confusion matrix + ROC curve.\"\"\"\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    result = compute_metrics(y_test, y_pred, y_prob, name)\n",
        "    result_dict = asdict(result)\n",
        "\n",
        "    # --- Metrics table ---\n",
        "    print(f\"\\n{'='*55}\")\n",
        "    print(f\"  {name}\")\n",
        "    print(f\"{'='*55}\")\n",
        "    for k, v in result_dict.items():\n",
        "        if k != \"model_name\":\n",
        "            print(f\"  {k:>10s} : {v:.4f}\")\n",
        "\n",
        "    # --- Classification report ---\n",
        "    print(f\"\\n  Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=[\"malignant\", \"benign\"]))\n",
        "\n",
        "    # --- Confusion Matrix + ROC Curve side by side ---\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 3.5))\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    ConfusionMatrixDisplay(cm, display_labels=[\"malignant\", \"benign\"]).plot(\n",
        "        ax=axes[0], cmap=\"Blues\", colorbar=False)\n",
        "    axes[0].set_title(f\"{name} \u2014 Confusion Matrix\")\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "    axes[1].plot(fpr, tpr, lw=2, label=f\"AUC = {result.auc:.4f}\")\n",
        "    axes[1].plot([0, 1], [0, 1], \"k--\", lw=1)\n",
        "    axes[1].set_xlabel(\"False Positive Rate\")\n",
        "    axes[1].set_ylabel(\"True Positive Rate\")\n",
        "    axes[1].set_title(f\"{name} \u2014 ROC Curve\")\n",
        "    axes[1].legend(loc=\"lower right\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return result_dict, pipeline\n",
        "\n",
        "# Storage for all results\n",
        "all_results = []\n",
        "all_models = {}\n",
        "\n",
        "print(\"Helpers defined \u2705\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 7a. Model 1 \u2014 Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "lr_pipe = Pipeline([\n",
        "    (\"preprocess\", build_preprocessor(feature_names, use_scaler=True)),\n",
        "    (\"classifier\", LogisticRegression(max_iter=2000, random_state=RANDOM_STATE)),\n",
        "])\n",
        "res, model = train_and_evaluate(\"Logistic Regression\", lr_pipe)\n",
        "all_results.append(res)\n",
        "all_models[\"Logistic Regression\"] = model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 7b. Model 2 \u2014 Decision Tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "dt_pipe = Pipeline([\n",
        "    (\"preprocess\", build_preprocessor(feature_names, use_scaler=False)),\n",
        "    (\"classifier\", DecisionTreeClassifier(random_state=RANDOM_STATE)),\n",
        "])\n",
        "res, model = train_and_evaluate(\"Decision Tree\", dt_pipe)\n",
        "all_results.append(res)\n",
        "all_models[\"Decision Tree\"] = model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 7c. Model 3 \u2014 K-Nearest Neighbors (kNN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "knn_pipe = Pipeline([\n",
        "    (\"preprocess\", build_preprocessor(feature_names, use_scaler=True)),\n",
        "    (\"classifier\", KNeighborsClassifier(n_neighbors=7)),\n",
        "])\n",
        "res, model = train_and_evaluate(\"kNN\", knn_pipe)\n",
        "all_results.append(res)\n",
        "all_models[\"kNN\"] = model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 7d. Model 4 \u2014 Naive Bayes (Gaussian)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "nb_pipe = Pipeline([\n",
        "    (\"preprocess\", build_preprocessor(feature_names, use_scaler=False)),\n",
        "    (\"classifier\", GaussianNB()),\n",
        "])\n",
        "res, model = train_and_evaluate(\"Naive Bayes\", nb_pipe)\n",
        "all_results.append(res)\n",
        "all_models[\"Naive Bayes\"] = model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 7e. Model 5 \u2014 Random Forest (Ensemble)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "rf_pipe = Pipeline([\n",
        "    (\"preprocess\", build_preprocessor(feature_names, use_scaler=False)),\n",
        "    (\"classifier\", RandomForestClassifier(n_estimators=400, random_state=RANDOM_STATE, n_jobs=-1)),\n",
        "])\n",
        "res, model = train_and_evaluate(\"Random Forest (Ensemble)\", rf_pipe)\n",
        "all_results.append(res)\n",
        "all_models[\"Random Forest (Ensemble)\"] = model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 7f. Model 6 \u2014 XGBoost (Ensemble)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "xgb_pipe = Pipeline([\n",
        "    (\"preprocess\", build_preprocessor(feature_names, use_scaler=False)),\n",
        "    (\"classifier\", XGBClassifier(\n",
        "        n_estimators=350, max_depth=4, learning_rate=0.05,\n",
        "        subsample=0.9, colsample_bytree=0.9,\n",
        "        random_state=RANDOM_STATE, objective=\"binary:logistic\",\n",
        "        eval_metric=\"logloss\", n_jobs=-1,\n",
        "    )),\n",
        "])\n",
        "res, model = train_and_evaluate(\"XGBoost (Ensemble)\", xgb_pipe)\n",
        "all_results.append(res)\n",
        "all_models[\"XGBoost (Ensemble)\"] = model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 8. Comparison Table \u2014 All 6 Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "metrics_df = pd.DataFrame(all_results)\n",
        "metrics_df = metrics_df.sort_values(\"accuracy\", ascending=False).reset_index(drop=True)\n",
        "metrics_df.style.format({\n",
        "    \"accuracy\": \"{:.4f}\", \"auc\": \"{:.4f}\", \"precision\": \"{:.4f}\",\n",
        "    \"recall\": \"{:.4f}\", \"f1\": \"{:.4f}\", \"mcc\": \"{:.4f}\",\n",
        "}).background_gradient(cmap=\"Greens\", subset=[\"accuracy\", \"auc\", \"precision\", \"recall\", \"f1\", \"mcc\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 9. Visual Comparison \u2014 Bar Charts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "metric_cols = [\"accuracy\", \"auc\", \"precision\", \"recall\", \"f1\", \"mcc\"]\n",
        "plot_df = metrics_df.set_index(\"model_name\")[metric_cols]\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(16, 8))\n",
        "colors = sns.color_palette(\"viridis\", len(plot_df))\n",
        "\n",
        "for ax, col in zip(axes.ravel(), metric_cols):\n",
        "    plot_df[col].sort_values().plot.barh(ax=ax, color=colors)\n",
        "    ax.set_title(col.upper(), fontsize=12, fontweight=\"bold\")\n",
        "    ax.set_xlim(0.8, 1.0)\n",
        "    ax.axvline(x=plot_df[col].mean(), color=\"red\", linestyle=\"--\", lw=1, label=\"mean\")\n",
        "    ax.legend(fontsize=8)\n",
        "\n",
        "plt.suptitle(\"Model Comparison \u2014 Evaluation Metrics\", fontsize=14, fontweight=\"bold\", y=1.01)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 10. ROC Curves \u2014 All Models Overlaid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(7, 5))\n",
        "for name, pipe in all_models.items():\n",
        "    y_prob = pipe.predict_proba(X_test)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "    auc_val = roc_auc_score(y_test, y_prob)\n",
        "    ax.plot(fpr, tpr, lw=2, label=f\"{name} (AUC={auc_val:.4f})\")\n",
        "\n",
        "ax.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
        "ax.set_xlabel(\"False Positive Rate\")\n",
        "ax.set_ylabel(\"True Positive Rate\")\n",
        "ax.set_title(\"ROC Curves \u2014 All Models\")\n",
        "ax.legend(loc=\"lower right\", fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 11. Save Models (.pkl files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "for name, pipe in all_models.items():\n",
        "    fname = name.lower().replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\") + \".pkl\"\n",
        "    joblib.dump(pipe, MODEL_DIR / fname)\n",
        "    print(f\"  Saved: {fname}\")\n",
        "\n",
        "# Save metrics CSV\n",
        "metrics_df.to_csv(MODEL_DIR / \"model_metrics.csv\", index=False)\n",
        "print(f\"  Saved: model_metrics.csv\")\n",
        "\n",
        "# Save confusion matrices JSON\n",
        "cm_dict = {}\n",
        "for name, pipe in all_models.items():\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    cm_dict[name] = confusion_matrix(y_test, y_pred).tolist()\n",
        "\n",
        "with open(MODEL_DIR / \"confusion_matrices.json\", \"w\") as f:\n",
        "    json.dump(cm_dict, f, indent=2)\n",
        "print(f\"  Saved: confusion_matrices.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 12. Save Train/Test CSVs & Metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# Save test data\n",
        "test_df = X_test.copy()\n",
        "test_df[\"target\"] = y_test.values\n",
        "test_df.to_csv(DATA_DIR / \"test_data.csv\", index=False)\n",
        "print(f\"  Saved: {DATA_DIR / 'test_data.csv'}\")\n",
        "\n",
        "# Save train data\n",
        "train_df = X_train.copy()\n",
        "train_df[\"target\"] = y_train.values\n",
        "train_df.to_csv(DATA_DIR / \"train_data.csv\", index=False)\n",
        "print(f\"  Saved: {DATA_DIR / 'train_data.csv'}\")\n",
        "\n",
        "# Save dataset metadata\n",
        "metadata = {\n",
        "    \"dataset_name\": \"UCI Breast Cancer Wisconsin (Diagnostic)\",\n",
        "    \"instances\": int(X.shape[0]),\n",
        "    \"features\": int(X.shape[1]),\n",
        "    \"target_names\": dataset.target_names.tolist(),\n",
        "    \"feature_names\": feature_names,\n",
        "    \"random_state\": RANDOM_STATE,\n",
        "}\n",
        "with open(MODEL_DIR / \"dataset_metadata.json\", \"w\") as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "print(f\"  Saved: dataset_metadata.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 13. Final Verification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "print(\"Saved artifacts:\\n\")\n",
        "for p in sorted(MODEL_DIR.glob(\"*\")):\n",
        "    if p.is_file() and not p.name.startswith(\"__\"):\n",
        "        print(f\"  model/{p.name:40s}  {p.stat().st_size/1024:>8.1f} KB\")\n",
        "for p in sorted(DATA_DIR.glob(\"*\")):\n",
        "    if p.is_file():\n",
        "        print(f\"  data/{p.name:41s}  {p.stat().st_size/1024:>8.1f} KB\")\n",
        "\n",
        "print(\"\\n\u2705 All done! Models trained, evaluated, and saved.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}