{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "# Breast Cancer Classification — Notebook Version\n",
    "\n",
    "This notebook mirrors `app.py` but uses native Python output instead of Streamlit.\n",
    "All six models are evaluated on the test split and compared side by side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / \"model\").exists() and (PROJECT_ROOT.parent / \"model\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "MODEL_DIR = PROJECT_ROOT / \"model\"\n",
    "DATA_DIR  = PROJECT_ROOT / \"data\"\n",
    "\n",
    "print(f\"MODEL_DIR : {MODEL_DIR}\")\n",
    "print(f\"DATA_DIR  : {DATA_DIR}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Model Map and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "MODEL_FILE_MAP = {\n",
    "    \"Logistic Regression\": \"logistic_regression.pkl\",\n",
    "    \"Decision Tree\": \"decision_tree.pkl\",\n",
    "    \"kNN\": \"knn.pkl\",\n",
    "    \"Naive Bayes\": \"naive_bayes.pkl\",\n",
    "    \"Random Forest (Ensemble)\": \"random_forest_ensemble.pkl\",\n",
    "    \"XGBoost (Ensemble)\": \"xgboost_ensemble.pkl\",\n",
    "}\n",
    "\n",
    "\n",
    "def load_model(name):\n",
    "    return joblib.load(MODEL_DIR / MODEL_FILE_MAP[name])\n",
    "\n",
    "\n",
    "def evaluate(y_true, y_pred, y_prob):\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_true, y_prob),\n",
    "        \"Precision\": precision_score(y_true, y_pred),\n",
    "        \"Recall\": recall_score(y_true, y_pred),\n",
    "        \"F1\": f1_score(y_true, y_pred),\n",
    "        \"MCC\": matthews_corrcoef(y_true, y_pred),\n",
    "    }"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Load Dataset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "with open(MODEL_DIR / \"dataset_metadata.json\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "print(f\"Dataset   : {meta['dataset_name']}\")\n",
    "print(f\"Samples   : {meta['instances']}\")\n",
    "print(f\"Features  : {meta['features']}\")\n",
    "print(f\"Classes   : {', '.join(meta['target_names'])}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "eval_df = pd.read_csv(DATA_DIR / \"test_data.csv\")\n",
    "feature_cols = [c for c in eval_df.columns if c != \"target\"]\n",
    "\n",
    "X_eval = eval_df[feature_cols]\n",
    "y_true = eval_df[\"target\"]\n",
    "\n",
    "print(f\"Test samples: {len(eval_df)}\")\n",
    "print(f\"Features    : {len(feature_cols)}\")\n",
    "eval_df.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Run All 6 Models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "all_metrics = []\n",
    "model_predictions = {}\n",
    "\n",
    "for name in MODEL_FILE_MAP:\n",
    "    model = load_model(name)\n",
    "    preds = model.predict(X_eval)\n",
    "    probs = model.predict_proba(X_eval)[:, 1]\n",
    "    model_predictions[name] = {\"preds\": preds, \"probs\": probs}\n",
    "\n",
    "    metrics = evaluate(y_true, preds, probs)\n",
    "    metrics[\"Model\"] = name\n",
    "    all_metrics.append(metrics)\n",
    "\n",
    "print(f\"Evaluated {len(all_metrics)} models.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## All-Model Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "metrics_df = metrics_df[[\"Model\", \"Accuracy\", \"AUC\", \"Precision\", \"Recall\", \"F1\", \"MCC\"]]\n",
    "metrics_df = metrics_df.sort_values(\"Accuracy\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "metrics_df.style.format(\n",
    "    {col: \"{:.4f}\" for col in [\"Accuracy\", \"AUC\", \"Precision\", \"Recall\", \"F1\", \"MCC\"]}\n",
    ").background_gradient(cmap=\"Greens\", subset=[\"Accuracy\", \"AUC\", \"F1\"])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Confusion Matrices (All Models)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, name in enumerate(MODEL_FILE_MAP):\n",
    "    cm = confusion_matrix(y_true, model_predictions[name][\"preds\"])\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=axes[i])\n",
    "    axes[i].set_title(name, fontsize=11)\n",
    "    axes[i].set_xlabel(\"Predicted\")\n",
    "    axes[i].set_ylabel(\"Actual\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## ROC Curves (All Models)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "for name in MODEL_FILE_MAP:\n",
    "    fpr, tpr, _ = roc_curve(y_true, model_predictions[name][\"probs\"])\n",
    "    auc_val = roc_auc_score(y_true, model_predictions[name][\"probs\"])\n",
    "    ax.plot(fpr, tpr, label=f\"{name} (AUC={auc_val:.3f})\")\n",
    "\n",
    "ax.plot([0, 1], [0, 1], \"k--\", alpha=0.4)\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "ax.set_title(\"ROC Comparison\")\n",
    "ax.legend(fontsize=9, loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Per-Model Deep Dive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "Change `SELECTED_MODEL` below to any of the six model names and re-run the cells that follow."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "SELECTED_MODEL = \"Logistic Regression\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "### Metrics for Selected Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "preds = model_predictions[SELECTED_MODEL][\"preds\"]\n",
    "probs = model_predictions[SELECTED_MODEL][\"probs\"]\n",
    "\n",
    "m = evaluate(y_true, preds, probs)\n",
    "for k, v in m.items():\n",
    "    print(f\"{k:>10s}: {v:.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "report = classification_report(y_true, preds, output_dict=True)\n",
    "pd.DataFrame(report).T"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "### Confusion Matrix for Selected Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "cm = confusion_matrix(y_true, preds)\n",
    "fig, ax = plt.subplots(figsize=(4.5, 3.5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=ax)\n",
    "ax.set_title(f\"Confusion Matrix — {SELECTED_MODEL}\")\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "### Prediction Preview"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "language": "python"
   },
   "source": [
    "preview = eval_df.copy()\n",
    "preview[\"prediction\"] = preds\n",
    "preview.head(20)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "To run the Streamlit web UI instead, use: `streamlit run app.py`"
   ]
  }
 ]
}