{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "# ML Assignment 2 \u2014 Interactive Model Evaluation (Notebook Version)\n",
        "\n",
        "This is the **notebook-friendly** version of `app.py`.  \n",
        "Streamlit `st.*` calls are replaced with standard notebook outputs so you can **see everything inline**.\n",
        "\n",
        "Run each cell sequentially to load models, evaluate, and visualise results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 1. Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    matthews_corrcoef,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    ConfusionMatrixDisplay,\n",
        ")\n",
        "\n",
        "print(\"All imports successful \u2705\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 2. Setup Paths & Load Metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "PROJECT_ROOT = Path.cwd()\n",
        "if not (PROJECT_ROOT / \"model\").exists() and (PROJECT_ROOT.parent / \"model\").exists():\n",
        "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
        "\n",
        "MODEL_DIR = PROJECT_ROOT / \"model\"\n",
        "DATA_DIR  = PROJECT_ROOT / \"data\"\n",
        "\n",
        "print(f\"MODEL_DIR : {MODEL_DIR}\")\n",
        "print(f\"DATA_DIR  : {DATA_DIR}\")\n",
        "\n",
        "# Load dataset metadata\n",
        "with open(MODEL_DIR / \"dataset_metadata.json\") as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "print(f\"\\nDataset   : {metadata['dataset_name']}\")\n",
        "print(f\"Instances : {metadata['instances']}\")\n",
        "print(f\"Features  : {metadata['features']}\")\n",
        "print(f\"Classes   : {metadata['target_names']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 3. Available Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "MODEL_FILE_MAP = {\n",
        "    \"Logistic Regression\": \"logistic_regression.pkl\",\n",
        "    \"Decision Tree\": \"decision_tree.pkl\",\n",
        "    \"kNN\": \"knn.pkl\",\n",
        "    \"Naive Bayes\": \"naive_bayes.pkl\",\n",
        "    \"Random Forest (Ensemble)\": \"random_forest_ensemble.pkl\",\n",
        "    \"XGBoost (Ensemble)\": \"xgboost_ensemble.pkl\",\n",
        "}\n",
        "\n",
        "print(\"Available models:\")\n",
        "for i, name in enumerate(MODEL_FILE_MAP, 1):\n",
        "    print(f\"  {i}. {name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 4. Load Precomputed Metrics Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "metrics_df = pd.read_csv(MODEL_DIR / \"model_metrics.csv\")\n",
        "metrics_df.style.format({\n",
        "    \"accuracy\": \"{:.4f}\", \"auc\": \"{:.4f}\", \"precision\": \"{:.4f}\",\n",
        "    \"recall\": \"{:.4f}\", \"f1\": \"{:.4f}\", \"mcc\": \"{:.4f}\",\n",
        "}).background_gradient(cmap=\"Greens\", subset=[\"accuracy\", \"auc\", \"precision\", \"recall\", \"f1\", \"mcc\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 5. Load Test Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(DATA_DIR / \"test_data.csv\")\n",
        "feature_cols = [c for c in test_df.columns if c != \"target\"]\n",
        "\n",
        "print(f\"Test samples : {len(test_df)}\")\n",
        "print(f\"Feature cols : {len(feature_cols)}\")\n",
        "test_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 6. Choose a Model to Evaluate\n",
        "\n",
        "Change the value of `SELECTED_MODEL` below to any of the 6 model names, then re-run this cell and the cells below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# \u2b07\ufe0f Change this to evaluate a different model\n",
        "SELECTED_MODEL = \"Logistic Regression\"\n",
        "\n",
        "model = joblib.load(MODEL_DIR / MODEL_FILE_MAP[SELECTED_MODEL])\n",
        "print(f\"Loaded: {SELECTED_MODEL}\")\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 7. Generate Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "X_eval = test_df[feature_cols]\n",
        "y_true = test_df[\"target\"]\n",
        "\n",
        "y_pred = model.predict(X_eval)\n",
        "y_prob = model.predict_proba(X_eval)[:, 1]\n",
        "\n",
        "print(f\"Predictions shape : {y_pred.shape}\")\n",
        "print(f\"First 10 predicted: {y_pred[:10].tolist()}\")\n",
        "print(f\"First 10 actual   : {y_true.values[:10].tolist()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 8. Evaluation Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "metrics = {\n",
        "    \"Accuracy\":  accuracy_score(y_true, y_pred),\n",
        "    \"AUC\":       roc_auc_score(y_true, y_prob),\n",
        "    \"Precision\": precision_score(y_true, y_pred),\n",
        "    \"Recall\":    recall_score(y_true, y_pred),\n",
        "    \"F1\":        f1_score(y_true, y_pred),\n",
        "    \"MCC\":       matthews_corrcoef(y_true, y_pred),\n",
        "}\n",
        "\n",
        "print(f\"\\n  Metrics for: {SELECTED_MODEL}\")\n",
        "print(f\"  {\"-\"*40}\")\n",
        "for k, v in metrics.items():\n",
        "    print(f\"  {k:>10s} : {v:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 9. Classification Report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "report = classification_report(y_true, y_pred, target_names=[\"malignant\", \"benign\"], output_dict=True)\n",
        "report_df = pd.DataFrame(report).transpose()\n",
        "report_df.style.format(\"{:.4f}\").background_gradient(cmap=\"Blues\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 10. Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_true, y_pred)\n",
        "fig, ax = plt.subplots(figsize=(5, 4))\n",
        "ConfusionMatrixDisplay(cm, display_labels=[\"malignant\", \"benign\"]).plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
        "ax.set_title(f\"{SELECTED_MODEL} \u2014 Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 11. ROC Curve\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "fig, ax = plt.subplots(figsize=(6, 4))\n",
        "ax.plot(fpr, tpr, lw=2, label=f\"AUC = {metrics['AUC']:.4f}\")\n",
        "ax.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
        "ax.set_xlabel(\"False Positive Rate\")\n",
        "ax.set_ylabel(\"True Positive Rate\")\n",
        "ax.set_title(f\"{SELECTED_MODEL} \u2014 ROC Curve\")\n",
        "ax.legend(loc=\"lower right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 12. Prediction Preview\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "preview = test_df.copy()\n",
        "preview[\"prediction\"] = y_pred\n",
        "preview[\"correct\"] = (preview[\"target\"] == preview[\"prediction\"])\n",
        "\n",
        "print(f\"Correct: {preview['correct'].sum()} / {len(preview)}\")\n",
        "preview.head(20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 13. Compare All Models on Test Set\n",
        "\n",
        "Loads every saved model and evaluates on the same test set so you can see a full side-by-side.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "all_eval = []\n",
        "for name, fname in MODEL_FILE_MAP.items():\n",
        "    pipe = joblib.load(MODEL_DIR / fname)\n",
        "    yp = pipe.predict(X_eval)\n",
        "    ypr = pipe.predict_proba(X_eval)[:, 1]\n",
        "    all_eval.append({\n",
        "        \"model_name\": name,\n",
        "        \"accuracy\": accuracy_score(y_true, yp),\n",
        "        \"auc\": roc_auc_score(y_true, ypr),\n",
        "        \"precision\": precision_score(y_true, yp),\n",
        "        \"recall\": recall_score(y_true, yp),\n",
        "        \"f1\": f1_score(y_true, yp),\n",
        "        \"mcc\": matthews_corrcoef(y_true, yp),\n",
        "    })\n",
        "\n",
        "compare_df = pd.DataFrame(all_eval).sort_values(\"accuracy\", ascending=False).reset_index(drop=True)\n",
        "compare_df.style.format({\n",
        "    \"accuracy\": \"{:.4f}\", \"auc\": \"{:.4f}\", \"precision\": \"{:.4f}\",\n",
        "    \"recall\": \"{:.4f}\", \"f1\": \"{:.4f}\", \"mcc\": \"{:.4f}\",\n",
        "}).background_gradient(cmap=\"Greens\", subset=[\"accuracy\", \"auc\", \"precision\", \"recall\", \"f1\", \"mcc\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "## 14. All Models \u2014 ROC Overlay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(7, 5))\n",
        "for name, fname in MODEL_FILE_MAP.items():\n",
        "    pipe = joblib.load(MODEL_DIR / fname)\n",
        "    ypr = pipe.predict_proba(X_eval)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_true, ypr)\n",
        "    auc_val = roc_auc_score(y_true, ypr)\n",
        "    ax.plot(fpr, tpr, lw=2, label=f\"{name} (AUC={auc_val:.4f})\")\n",
        "\n",
        "ax.plot([0, 1], [0, 1], \"k--\", lw=1)\n",
        "ax.set_xlabel(\"False Positive Rate\")\n",
        "ax.set_ylabel(\"True Positive Rate\")\n",
        "ax.set_title(\"ROC Curves \u2014 All Models\")\n",
        "ax.legend(loc=\"lower right\", fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "---\n",
        "\n",
        "\u2705 **Done!** You have verified all models interactively.  \n",
        "To run the actual Streamlit web UI, use: `streamlit run app.py`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}